{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise DocumentStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.document_stores import InMemoryDocumentStore\n",
    "\n",
    "document_store = InMemoryDocumentStore(use_bm25=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use TextIndexingPipeline to convert text files into Haystack Document objects and write them into DocumentStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_dir = \"data/bulletins\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from haystack.pipelines.standard_pipelines import TextIndexingPipeline\n",
    "\n",
    "files_to_index = [doc_dir + \"/\" + file for file in os.listdir(doc_dir)]\n",
    "indexing_pipeline = TextIndexingPipeline(document_store=document_store)\n",
    "indexing_pipeline.run_batch(file_paths=files_to_index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise a Retriever. This sifts through documents and returns the best documents relative to the question. This uses the BM25 algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.nodes import BM25Retriever\n",
    "\n",
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise the reader. This reads through the documents and returns the best answer to the question. This uses the RoBERTa question answering model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.nodes import FARMReader\n",
    "\n",
    "reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\", use_gpu=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a ready made pipeline called ExtractiveQAPipeline on the documents to find answers for our questions in the Wikipedia articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.pipelines import ExtractiveQAPipeline\n",
    "\n",
    "pipe = ExtractiveQAPipeline(reader, retriever)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Asking a question**\n",
    "\n",
    "The pipeline is now ready to ask questions. We can ask a question by using the pipeline's run function to retrieve a prediction. We can also specify how many answers we want to get back with top_k_retriever and top_k_reader.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What proportion of firms think their costs will go up?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ff1f15bfe804b14ad7b6b33e2e139e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prediction = pipe.run(\n",
    "    query=query, \n",
    "    params={\n",
    "        \"Retriever\": {\"top_k\": 1}, # pick single document\n",
    "        \"Reader\": {\"top_k\": 1} # only one answer\n",
    "    }\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Query: What proportion of firms think their costs will go up?'\n",
      "'Answers:'\n",
      "[   <Answer {'answer': 'more than one in six (18%) trading businesses expected to raise the prices of goods or services they sell in June 2023, down from 23% for May 2023. While the proportion of trading businesses that reported they expect prices will stay the same increased by 7 percentage points to 60%', 'type': 'extractive', 'score': 0.024097830057144165, 'context': 'more than one in six (18%) trading businesses expected to raise the prices of goods or services they sell in June 2023, down from 23% for May 2023. While the proportion of trading businesses that reported they expect prices will stay the same increased by 7 percentage points to 60%', 'offsets_in_document': [{'start': 722, 'end': 1004}], 'offsets_in_context': [{'start': 0, 'end': 282}], 'document_ids': ['d7ebe3df07971ba4333e9ca06b5126a8'], 'meta': {'_split_id': 3}}>]\n"
     ]
    }
   ],
   "source": [
    "from haystack.utils import print_answers\n",
    "\n",
    "print_answers(\n",
    "    prediction,\n",
    "    details=\"all\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e64d64c086741ef4a01b1973e253f24b05e5f8201e043641bb65f7528e84d7ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
